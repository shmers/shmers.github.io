[
  {
    "objectID": "projects/pricing-risk.html",
    "href": "projects/pricing-risk.html",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "",
    "text": "This project demonstrates an end‚Äëto‚Äëend approach to designing, validating, and evaluating a predictive risk model in a pricing context. The focus is on modelling judgement, feature design, and validation rather than maximising headline performance metrics. Here I‚Äôve intentionally generated a challenging synthetic dataset to demonstrate the modelling options available in typical insurance pricing contexts.\nDisclaimer to CMA: All data used in this project is fully synthetic (generated right before your very eyes, no less) and does not reflect any proprietary datasets."
  },
  {
    "objectID": "projects/pricing-risk.html#overview",
    "href": "projects/pricing-risk.html#overview",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "",
    "text": "This project demonstrates an end‚Äëto‚Äëend approach to designing, validating, and evaluating a predictive risk model in a pricing context. The focus is on modelling judgement, feature design, and validation rather than maximising headline performance metrics. Here I‚Äôve intentionally generated a challenging synthetic dataset to demonstrate the modelling options available in typical insurance pricing contexts.\nDisclaimer to CMA: All data used in this project is fully synthetic (generated right before your very eyes, no less) and does not reflect any proprietary datasets."
  },
  {
    "objectID": "projects/pricing-risk.html#problem-statement",
    "href": "projects/pricing-risk.html#problem-statement",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe consider a hypothetical insurance‚Äëstyle problem where the goal is to predict the probability of a high‚Äëcost loss event for individual policies, based on customer, asset, and contextual features.\nKey challenges include:\n\nClass imbalance\nFeature leakage\nModel interpretability vs performance trade‚Äëoffs"
  },
  {
    "objectID": "projects/pricing-risk.html#data-generation",
    "href": "projects/pricing-risk.html#data-generation",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Data Generation",
    "text": "Data Generation\nWe generate a synthetic dataset designed to capture common characteristics of real‚Äëworld consumer risk portfolios:\n\nSkewed loss distribution\nCorrelated features\nRare but severe outcomes\n\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n\nn = 20000\n\ndata = pd.DataFrame({\n    \"vehicle_value\": np.random.lognormal(mean=10, sigma=0.5, size=n),\n    \"driver_age\": np.random.randint(18, 80, size=n),\n    \"annual_mileage\": np.random.gamma(shape=2, scale=6000, size=n),\n    \"urban_density\": np.random.beta(2, 5, size=n),\n    \"prior_claims\": np.random.poisson(0.3, size=n)\n})\n\nlogit = (\n    -7.0\n    + 0.000002 * data[\"vehicle_value\"]\n    - 0.015 * data[\"driver_age\"]\n    + 0.00004 * data[\"annual_mileage\"]\n    + 1.5 * data[\"urban_density\"]\n    + 0.6 * data[\"prior_claims\"]\n)\n\nprob = 1 / (1 + np.exp(-logit))\ndata[\"high_cost_claim\"] = np.random.binomial(1, prob)\n\ndata.head()\n\n\n\n\n\n\n\n\nvehicle_value\ndriver_age\nannual_mileage\nurban_density\nprior_claims\nhigh_cost_claim\n\n\n\n\n0\n12799.798382\n69\n3883.837719\n0.171791\n0\n0\n\n\n1\n36267.333927\n26\n17499.414406\n0.065992\n0\n0\n\n\n2\n25374.226904\n54\n5726.365121\n0.222687\n1\n0\n\n\n3\n10371.870313\n78\n10299.632113\n0.683120\n0\n0\n\n\n4\n16493.140959\n74\n1743.097292\n0.366287\n0\n0"
  },
  {
    "objectID": "projects/pricing-risk.html#feature-engineering",
    "href": "projects/pricing-risk.html#feature-engineering",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nWe‚Äôll keep feature engineering very basic here, banding features for interpretability and stability, with transformations / interactions chosen to reflect common approaches used in pricing and risk models, rather than going for model complexity straight away.\nKey considerations include: - Handling heavy‚Äëtailed distributions - Capturing non‚Äëlinear effects - Maintaining transparency for downstream stakeholders\n\nimport numpy as np\nimport pandas as pd\n\ndf = data.copy()\n\n# Log-transform heavy-tailed monetary values\ndf[\"log_vehicle_value\"] = np.log(df[\"vehicle_value\"])\n\n# Mileage exposure bands\ndf[\"mileage_band\"] = pd.cut(\n    df[\"annual_mileage\"],\n    bins=[0, 5000, 10000, 15000, 25000, np.inf],\n    labels=False\n)\n\n# Age bands to capture non-linear risk patterns\ndf[\"age_band\"] = pd.cut(\n    df[\"driver_age\"],\n    bins=[17, 25, 35, 50, 65, 100],\n    labels=False\n)\n\n# Interaction: high value asset in dense urban areas\ndf[\"urban_value_interaction\"] = (\n    df[\"urban_density\"] * df[\"log_vehicle_value\"]\n)\n\nengineered_features = [\n    \"log_vehicle_value\",\n    \"mileage_band\",\n    \"age_band\",\n    \"urban_density\",\n    \"prior_claims\",\n    \"urban_value_interaction\"\n]\n\nX = df[engineered_features]\ny = df[\"high_cost_claim\"]\n\nX.head()\n\n\n\n\n\n\n\n\nlog_vehicle_value\nmileage_band\nage_band\nurban_density\nprior_claims\nurban_value_interaction\n\n\n\n\n0\n9.457185\n0\n4\n0.171791\n0\n1.624659\n\n\n1\n10.498673\n3\n1\n0.065992\n0\n0.692828\n\n\n2\n10.141489\n1\n3\n0.222687\n1\n2.258378\n\n\n3\n9.246853\n2\n4\n0.683120\n0\n6.316712\n\n\n4\n9.710700\n0\n4\n0.366287\n0\n3.556906"
  },
  {
    "objectID": "projects/pricing-risk.html#modelling-approach",
    "href": "projects/pricing-risk.html#modelling-approach",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Modelling Approach",
    "text": "Modelling Approach\nAs a baseline, let‚Äôs fit a logistic regression model using the engineered features. Logistic regression provides a transparent and well‚Äëunderstood benchmark, allowing coefficient‚Äëlevel interpretation and serving as a reference point for more complex models later on.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n# Train / test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Standardise features for logistic regression\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Fit model\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train_scaled, y_train)\n\n# Predict probabilities\ny_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n\nroc_auc = roc_auc_score(y_test, y_pred_proba)\nroc_auc\n\n0.6906500333778371\n\n\nTo contextualise this value, it‚Äôs worth bearing in mind that random guessing would achieve an ROC AUC of 0.50, 0.70+ would indicate strong separation, and 1.0 a perfect model.\nOur value shows that the model could be stronger, but is still better than random. In a real insurance context we would have at least a few different options to explore here to improve the model:\n\nNon‚Äëlinear effects\nInteraction terms\nBehavioural history depth\nLatent segments\n\n‚Ä¶ but we don‚Äôt want to end up turning this scenario into a toy problem; this isn‚Äôt Kaggle.\nLet‚Äôs keep the focus on evaluation and calibration, as we‚Äôre not interested in maximising performance just yet, we‚Äôll save that fun for later when we see what gradient boosting can do."
  },
  {
    "objectID": "projects/pricing-risk.html#coefficient-interpretation",
    "href": "projects/pricing-risk.html#coefficient-interpretation",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Coefficient Interpretation",
    "text": "Coefficient Interpretation\nInspecting model coefficients quickly helps us validate whether learned relationships align with domain expectations.\n\ncoef_df = pd.DataFrame({\n    \"feature\": engineered_features,\n    \"coefficient\": log_reg.coef_[0]\n}).sort_values(by=\"coefficient\", ascending=False)\n\ncoef_df\n\n\n\n\n\n\n\n\nfeature\ncoefficient\n\n\n\n\n4\nprior_claims\n0.524268\n\n\n1\nmileage_band\n0.360799\n\n\n3\nurban_density\n0.180719\n\n\n5\nurban_value_interaction\n0.175511\n\n\n0\nlog_vehicle_value\n0.089934\n\n\n2\nage_band\n-0.291732\n\n\n\n\n\n\n\nObviously since we created the label ourselves using a logit distribution we‚Äôre not surprised to see the original coefficient values reflected here in the logistic regression model. In reality at this stage of modelling we wouldn‚Äôt be expecting to deduce anything profound yet from an unsophisticated view such as this, but we might opt to construct a rough-and-ready feedback loop with our feature engineering choices for rapid prototyping.\nIn my experience this type of feedback loop can pay dividends for improving the efficacy of engineered features without too much additional time or resource, and works especially well when the chosen evaluation metrics are thoughtfully chosen, such as measures of marginal predictive skill contribution / model uplift on a feature level basis."
  },
  {
    "objectID": "projects/pricing-risk.html#validation-and-calibration",
    "href": "projects/pricing-risk.html#validation-and-calibration",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Validation and Calibration",
    "text": "Validation and Calibration\nIn risk‚Äëbased decision systems, well‚Äëcalibrated probabilities are often as important as ranking performance. We therefore examine calibration behaviour alongside discrimination metrics.\n\nfrom sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\nprob_true, prob_pred = calibration_curve(\n    y_test,\n    y_pred_proba,\n    n_bins=10,\n    strategy=\"quantile\"\n)\n\nplt.figure(figsize=(6, 6))\nplt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfect calibration\")\nplt.xlabel(\"Predicted probability\")\nplt.ylabel(\"Observed frequency\")\nplt.title(\"Calibration Curve\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNot much to see here, it seems‚Ä¶ We‚Äôve chosen a difficult problem to model, that of a rare-event setting, such as a large 3rd party BI claim in UK motor insurance. Let‚Äôs see how far we‚Äôd need to zoom in to see what‚Äôs going on.\n\nHistogram of probabilities\n\nplt.figure(figsize=(6, 4))\nplt.hist(y_pred_proba, bins=50)\nplt.xlabel(\"Predicted probability\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Predicted Probabilities\")\nplt.show()\n\n\n\n\n\n\n\n\nSo looks like probabilities are only significantly allocated between 0% and 1%\n\nZoomed in to 0 - 1%\n\nplt.figure(figsize=(6, 6))\nplt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\nplt.plot([0, 0.1], [0, 0.1], linestyle=\"--\", label=\"Perfect calibration\")\n\nplt.xlim(0, 0.01)\nplt.ylim(0, 0.01)\n\nplt.xlabel(\"Predicted probability\")\nplt.ylabel(\"Observed frequency\")\nplt.title(\"Calibration Curve (Zoomed: 0‚Äì1%)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe calibration curve assesses whether predicted probabilities correspond to observed event frequencies. Predictions that lie close to the diagonal indicate well‚Äëcalibrated probabilities, while systematic deviations highlight over‚Äë or under‚Äëconfidence.\nIn rare‚Äëevent settings, calibration at higher predicted probabilities can be noisy due to limited sample sizes. Nevertheless, this analysis provides important insight into whether predicted risks are suitable for downstream decision‚Äëmaking or require post‚Äëhoc calibration.\nHere we can see that the model seems passable at assigning probabilities that aren‚Äôt too far off of the mark, generally speaking. If we had grouped more aggressively we might not see as much noise, but I tend to prefer erring on the side of seeing the noise and reassuring myself that it truly is noise, and not a systematic calibration error that may need further thought. Here the points seem fairly randomly distributed around the perfect calibration line, so no strong need for post-hoc calibration necessarily yet, but there‚Äôs more we can do to answer this concern.\n\n\nBrier Score\nThe Brier score is defined as $ (p_i - y_i)^2 $ averaged over observations. In other words the average squared error in the predicted probabilities.\nThe Brier score is a proper scoring rule that measures the accuracy of probabilistic predictions. For a binary outcome, it is defined as:\n\\[\n\\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - y_i)^2\n\\]\nwhere \\(p_i\\) is the predicted probability of the event and \\(y_i\\) is the observed outcome.\n\nfrom sklearn.metrics import brier_score_loss\n\nbrier = brier_score_loss(y_test, y_pred_proba)\nbrier\n\n0.0013324230061909923\n\n\nLower Brier scores indicate better overall probability accuracy, penalising both over‚Äëconfident and under‚Äëconfident predictions. Unlike ROC‚ÄëAUC, the Brier score directly reflects the quality of predicted probabilities, making it particularly relevant in risk‚Äëbased decision systems.\nAlthough this value looks fairly low in absolute terms, let‚Äôs bear in mind that we‚Äôll need to check against a logical benchmark to know how good a score this is, and whether we should be reassured or alarmed at its value.\n\n\n\nBrier Skill Score\nInterpreting the Brier Score is easier with a relative measure like the Brier Skill Score (BSS), which measures the improvement in probabilistic accuracy relative to a reference forecast. It is defined as:\n\\[\n\\text{BSS} = 1 - \\frac{\\text{Brier}_{\\text{model}}}{\\text{Brier}_{\\text{ref}}}\n\\]\nIn this example, the reference forecast is a constant base‚Äërate model that predicts the empirical event rate for all observations. For rare event scenarios such as the synthetically generated dataset we‚Äôre looking at, this is somewhat difficult to beat, so let‚Äôs see how we did:\n\nimport numpy as np\nfrom sklearn.metrics import brier_score_loss\n\n# Reference forecast: base rate\nbase_rate = y_train.mean()\ny_ref = np.full_like(y_test, fill_value=base_rate, dtype=float)\n\nbrier_ref = brier_score_loss(y_test, y_ref)\nbrier_model = brier_score_loss(y_test, y_pred_proba)\n\nbrier_skill_score = 1 - (brier_model / brier_ref)\nbrier_skill_score\n\n-0.0006497525279884897\n\n\nA positive Brier Skill Score indicates that the model improves probabilistic accuracy relative to the base‚Äërate benchmark. Negative means we‚Äôre predicting worse than the default base-rate prediction. The Brier Skill Score contextualises absolute probability error by comparing against a naive baseline. In rare‚Äëevent problems, even modest positive skill represents meaningful improvement over base‚Äërate forecasting.\nHowever, it‚Äôs important to note that a negative Brier Skill Score does not necessarily mean the model is performing badly. Just that it is not yet calibrated properly. Earlier we saw an okay ROC AUC value, so let‚Äôs consider post-hoc calibration options to see whether we can optimise our probabilities without deteriorating the ranking performance that we were otherwise happy with."
  },
  {
    "objectID": "projects/pricing-risk.html#posthoc-calibration-comparison",
    "href": "projects/pricing-risk.html#posthoc-calibration-comparison",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Post‚ÄëHoc Calibration Comparison",
    "text": "Post‚ÄëHoc Calibration Comparison\nTo assess whether probability calibration improves probabilistic accuracy, we compare the uncalibrated model against two post‚Äëhoc calibration approaches: Platt scaling and isotonic regression.\n\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import brier_score_loss\nimport pandas as pd\n\n# Base model (same specification)\nbase_model = LogisticRegression(max_iter=1000)\n\n# Platt scaling (sigmoid)\nplatt_model = CalibratedClassifierCV(\n    base_model,\n    method=\"sigmoid\",\n    cv=5\n)\n\n# Isotonic calibration\nisotonic_model = CalibratedClassifierCV(\n    base_model,\n    method=\"isotonic\",\n    cv=5\n)\n\n# Fit calibrated models\nplatt_model.fit(X_train_scaled, y_train)\nisotonic_model.fit(X_train_scaled, y_train)\n\n# Predict probabilities\ny_platt = platt_model.predict_proba(X_test_scaled)[:, 1]\ny_isotonic = isotonic_model.predict_proba(X_test_scaled)[:, 1]\n\n# Base-rate reference\nbase_rate = y_train.mean()\ny_ref = np.full_like(y_test, base_rate, dtype=float)\n\n# Compute Brier scores\nresults = pd.DataFrame({\n    \"Model\": [\"Uncalibrated\", \"Platt scaling\", \"Isotonic\"],\n    \"Brier score\": [\n        brier_score_loss(y_test, y_pred_proba),\n        brier_score_loss(y_test, y_platt),\n        brier_score_loss(y_test, y_isotonic)\n    ]\n})\n\nresults\n\n\n\n\n\n\n\n\nModel\nBrier score\n\n\n\n\n0\nUncalibrated\n0.001332\n\n\n1\nPlatt scaling\n0.001331\n\n\n2\nIsotonic\n0.001331\n\n\n\n\n\n\n\n\nCompute Brier Skill scores\n\nresults[\"Brier Skill Score\"] = 1 - (\n    results[\"Brier score\"] / brier_score_loss(y_test, y_ref)\n)\n\nresults\n\n\n\n\n\n\n\n\nModel\nBrier score\nBrier Skill Score\n\n\n\n\n0\nUncalibrated\n0.001332\n-0.000650\n\n\n1\nPlatt scaling\n0.001331\n0.000286\n\n\n2\nIsotonic\n0.001331\n0.000217\n\n\n\n\n\n\n\nPost‚Äëhoc calibration improves probability scaling relative to the uncalibrated model, reducing overall Brier score. We can see here that both Platt scaling and Isotonic calibration improved our Brier Skill score into a slight positive result, from negative originally. This confirms our suspicion that our model probabilities were suffering from a calibration issue. Let‚Äôs redraw our calibration curve and see the difference."
  },
  {
    "objectID": "projects/pricing-risk.html#calibration-curves-before-and-after-calibration",
    "href": "projects/pricing-risk.html#calibration-curves-before-and-after-calibration",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Calibration Curves: Before and After Calibration",
    "text": "Calibration Curves: Before and After Calibration\nTo visualise the effect of post‚Äëhoc calibration, we compare calibration curves for the uncalibrated model and the calibrated variants. Given the rarity of the event, the plot focuses on the probability range where most predictions lie.\n\nfrom sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\n# Compute calibration curves\nprob_true_uncal, prob_pred_uncal = calibration_curve(\n    y_test, y_pred_proba, n_bins=5, strategy=\"quantile\"\n)\n\nprob_true_platt, prob_pred_platt = calibration_curve(\n    y_test, y_platt, n_bins=5, strategy=\"quantile\"\n)\n\nprob_true_iso, prob_pred_iso = calibration_curve(\n    y_test, y_isotonic, n_bins=5, strategy=\"quantile\"\n)\n\n# Plot\nplt.figure(figsize=(7, 7))\n\nplt.plot(prob_pred_uncal, prob_true_uncal, marker=\"o\", label=\"Uncalibrated\")\nplt.plot(prob_pred_platt, prob_true_platt, marker=\"o\", label=\"Platt scaling\")\nplt.plot(prob_pred_iso, prob_true_iso, marker=\"o\", label=\"Isotonic\")\n\n# Perfect calibration reference\nplt.plot([0, 0.005], [0, 0.005], linestyle=\"--\", color=\"black\", label=\"Perfect calibration\")\n\n# Zoomed axes for rare-event setting\nplt.xlim(0, 0.005)\nplt.ylim(0, 0.005)\n\nplt.xlabel(\"Predicted probability\")\nplt.ylabel(\"Observed frequency\")\nplt.title(\"Calibration Curves (Zoomed: 0‚Äì0.5%)\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nPost‚Äëhoc calibration improves the alignment between predicted probabilities and observed event frequencies. Platt scaling provides a smooth adjustment that improves probability reliability, while isotonic calibration offers greater flexibility at the cost of increased variance in sparse regions. In rare‚Äëevent settings, these trade‚Äëoffs must be balanced thoughtfully."
  },
  {
    "objectID": "projects/pricing-risk.html#todo---gradient-boosting",
    "href": "projects/pricing-risk.html#todo---gradient-boosting",
    "title": "Designing a Predictive Risk Model for High-Value Assets",
    "section": "Todo - Gradient Boosting",
    "text": "Todo - Gradient Boosting"
  },
  {
    "objectID": "projects/geospatial-risk.html",
    "href": "projects/geospatial-risk.html",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "",
    "text": "This project demonstrates how geospatial context can be incorporated into predictive risk models using aggregated, non‚Äëidentifying spatial features. The emphasis is on feature construction, spatial smoothing, and avoiding information leakage rather than on fine‚Äëgrained location data.\nAll data used in this project is synthetic or publicly available."
  },
  {
    "objectID": "projects/geospatial-risk.html#overview",
    "href": "projects/geospatial-risk.html#overview",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "",
    "text": "This project demonstrates how geospatial context can be incorporated into predictive risk models using aggregated, non‚Äëidentifying spatial features. The emphasis is on feature construction, spatial smoothing, and avoiding information leakage rather than on fine‚Äëgrained location data.\nAll data used in this project is synthetic or publicly available."
  },
  {
    "objectID": "projects/geospatial-risk.html#problem-statement",
    "href": "projects/geospatial-risk.html#problem-statement",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Problem Statement",
    "text": "Problem Statement\nWe consider a hypothetical risk prediction problem where event likelihood varies spatially due to underlying environmental, behavioural, or socio‚Äëeconomic factors. The goal is to construct geospatial features that capture this variation in a stable and privacy‚Äëpreserving way.\nKey challenges include:\n\nSpatial leakage\nSparsity in fine‚Äëgrained locations\nBalancing resolution with robustness"
  },
  {
    "objectID": "projects/geospatial-risk.html#synthetic-spatial-data",
    "href": "projects/geospatial-risk.html#synthetic-spatial-data",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Synthetic Spatial Data",
    "text": "Synthetic Spatial Data\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\n\nn = 20000\n\n# Synthetic latitude / longitude (approximate UK bounding box)\nlat = np.random.uniform(50.0, 55.5, n)\nlon = np.random.uniform(-5.5, 1.8, n)\n\ndf = pd.DataFrame({\n    \"latitude\": lat,\n    \"longitude\": lon\n})\n\ndf.head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\n\n\n\n\n0\n53.830581\n-4.868714\n\n\n1\n51.573766\n-1.235600\n\n\n2\n51.247683\n-2.048911\n\n\n3\n53.032231\n-0.370855\n\n\n4\n53.957079\n-2.130276"
  },
  {
    "objectID": "projects/geospatial-risk.html#create-spatial-clusters-of-higher-risk",
    "href": "projects/geospatial-risk.html#create-spatial-clusters-of-higher-risk",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Create spatial clusters of higher risk",
    "text": "Create spatial clusters of higher risk\n\nrisk_centers = [\n    (51.5, -0.1),   # London\n    (53.5, -2.3),   # Manchester\n    (52.5, -1.9)    # Birmingham\n]\n\nrisk = np.zeros(n)\n\nfor lat_c, lon_c in risk_centers:\n    dist = np.sqrt((df[\"latitude\"] - lat_c)**2 + (df[\"longitude\"] - lon_c)**2)\n    risk += np.exp(-dist / 0.5)\n\n# Normalise\nrisk = risk / risk.max()\n\ndf[\"spatial_risk\"] = risk\n\ndf.head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nspatial_risk\n\n\n\n\n0\n53.830581\n-4.868714\n0.006423\n\n\n1\n51.573766\n-1.235600\n0.195246\n\n\n2\n51.247683\n-2.048911\n0.099452\n\n\n3\n53.032231\n-0.370855\n0.092219\n\n\n4\n53.957079\n-2.130276\n0.387473"
  },
  {
    "objectID": "projects/geospatial-risk.html#event-generation",
    "href": "projects/geospatial-risk.html#event-generation",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Event Generation",
    "text": "Event Generation\n\nlogit = -4.0 + 2.0 * df[\"spatial_risk\"]\nprob = 1 / (1 + np.exp(-logit))\n\ndf[\"event\"] = np.random.binomial(1, prob)\n\ndf[\"event\"].mean()\n\nnp.float64(0.02315)"
  },
  {
    "objectID": "projects/geospatial-risk.html#spatial-binning",
    "href": "projects/geospatial-risk.html#spatial-binning",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Spatial Binning",
    "text": "Spatial Binning\nTo incorporate spatial context in a robust and privacy‚Äëpreserving way, we discretise latitude and longitude into coarse spatial bins. This reduces sensitivity to exact coordinates while enabling aggregation of local risk signals.\n\n# Define bin sizes (degrees)\nlat_bin_size = 0.25\nlon_bin_size = 0.25\n\ndf[\"lat_bin\"] = (df[\"latitude\"] / lat_bin_size).astype(int)\ndf[\"lon_bin\"] = (df[\"longitude\"] / lon_bin_size).astype(int)\n\n# Combine into a single spatial cell identifier\ndf[\"spatial_cell\"] = df[\"lat_bin\"].astype(str) + \"_\" + df[\"lon_bin\"].astype(str)\n\ndf[[\"latitude\", \"longitude\", \"spatial_cell\"]].head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nspatial_cell\n\n\n\n\n0\n53.830581\n-4.868714\n215_-19\n\n\n1\n51.573766\n-1.235600\n206_-4\n\n\n2\n51.247683\n-2.048911\n204_-8\n\n\n3\n53.032231\n-0.370855\n212_-1\n\n\n4\n53.957079\n-2.130276\n215_-8"
  },
  {
    "objectID": "projects/geospatial-risk.html#spatial-aggregation",
    "href": "projects/geospatial-risk.html#spatial-aggregation",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Spatial Aggregation",
    "text": "Spatial Aggregation\nWithin each spatial cell, we compute aggregated statistics that summarise local event behaviour. These aggregates form the basis of geospatial risk features.\n\ncell_stats = (\n    df.groupby(\"spatial_cell\")\n      .agg(\n          cell_event_rate=(\"event\", \"mean\"),\n          cell_event_count=(\"event\", \"sum\"),\n          cell_exposure=(\"event\", \"count\")\n      )\n      .reset_index()\n)\n\ncell_stats.head()\n\n\n\n\n\n\n\n\nspatial_cell\ncell_event_rate\ncell_event_count\ncell_exposure\n\n\n\n\n0\n200_-1\n0.030303\n1\n33\n\n\n1\n200_-10\n0.000000\n0\n39\n\n\n2\n200_-11\n0.041667\n1\n24\n\n\n3\n200_-12\n0.000000\n0\n39\n\n\n4\n200_-13\n0.055556\n2\n36"
  },
  {
    "objectID": "projects/geospatial-risk.html#smoothing-sparse-spatial-estimates",
    "href": "projects/geospatial-risk.html#smoothing-sparse-spatial-estimates",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Smoothing Sparse Spatial Estimates",
    "text": "Smoothing Sparse Spatial Estimates\nRaw cell event rates can be noisy when exposure is low.\nWe‚Äôll apply empirical Bayes‚Äìstyle smoothing.\nTo reduce variance in low‚Äëexposure cells, we apply simple Bayesian smoothing by shrinking cell‚Äëlevel event rates toward the global mean.\n\n# Global event rate\nglobal_rate = df[\"event\"].mean()\n\n# Smoothing strength (pseudo-counts)\nalpha = 50\n\ncell_stats[\"smoothed_event_rate\"] = (\n    (cell_stats[\"cell_event_rate\"] * cell_stats[\"cell_exposure\"] + alpha * global_rate)\n    / (cell_stats[\"cell_exposure\"] + alpha)\n)\n\ncell_stats.head()\n\n\n\n\n\n\n\n\nspatial_cell\ncell_event_rate\ncell_event_count\ncell_exposure\nsmoothed_event_rate\n\n\n\n\n0\n200_-1\n0.030303\n1\n33\n0.025994\n\n\n1\n200_-10\n0.000000\n0\n39\n0.013006\n\n\n2\n200_-11\n0.041667\n1\n24\n0.029155\n\n\n3\n200_-12\n0.000000\n0\n39\n0.013006\n\n\n4\n200_-13\n0.055556\n2\n36\n0.036715"
  },
  {
    "objectID": "projects/geospatial-risk.html#constructing-geospatial-features",
    "href": "projects/geospatial-risk.html#constructing-geospatial-features",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Constructing Geospatial Features",
    "text": "Constructing Geospatial Features\nWe attach the aggregated spatial features back to the individual‚Äëlevel dataset for use in downstream modelling.\n\ndf = df.merge(\n    cell_stats[[\"spatial_cell\", \"smoothed_event_rate\"]],\n    on=\"spatial_cell\",\n    how=\"left\"\n)\n\ndf[[\"latitude\", \"longitude\", \"smoothed_event_rate\"]].head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nsmoothed_event_rate\n\n\n\n\n0\n53.830581\n-4.868714\n0.014652\n\n\n1\n51.573766\n-1.235600\n0.027660\n\n\n2\n51.247683\n-2.048911\n0.041006\n\n\n3\n53.032231\n-0.370855\n0.028019\n\n\n4\n53.957079\n-2.130276\n0.028388\n\n\n\n\n\n\n\n\n\nfrom shapely.geometry import Point\nimport geopandas as gpd\n\ngeometry = [\n    Point(lon, lat)\n    for lon, lat in zip(df[\"longitude\"], df[\"latitude\"])\n]\n\ngdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\ngdf_bng = gdf.to_crs(epsg=27700)"
  },
  {
    "objectID": "projects/geospatial-risk.html#visualising-raw-spatial-events",
    "href": "projects/geospatial-risk.html#visualising-raw-spatial-events",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Visualising Raw Spatial Events",
    "text": "Visualising Raw Spatial Events\nWe begin by visualising the spatial distribution of events to understand broad geographic patterns and clustering behaviour.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7, 7))\n\nplt.scatter(\n    df[\"longitude\"],\n    df[\"latitude\"],\n    c=df[\"event\"],\n    cmap=\"coolwarm\",\n    alpha=0.3,\n    s=10\n)\n\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"Spatial Distribution of Events\")\nplt.colorbar(label=\"Event indicator\")\nplt.show()"
  },
  {
    "objectID": "projects/geospatial-risk.html#visualising-smoothed-spatial-risk",
    "href": "projects/geospatial-risk.html#visualising-smoothed-spatial-risk",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Visualising Smoothed Spatial Risk",
    "text": "Visualising Smoothed Spatial Risk\nAggregated and smoothed spatial features provide a clearer view of underlying geographic risk patterns.\n\n# Use cell-level centroids for plotting\ncell_stats[\"lat_center\"] = (\n    cell_stats[\"spatial_cell\"]\n    .str.split(\"_\")\n    .str[0]\n    .astype(int) * lat_bin_size\n)\n\ncell_stats[\"lon_center\"] = (\n    cell_stats[\"spatial_cell\"]\n    .str.split(\"_\")\n    .str[1]\n    .astype(int) * lon_bin_size\n)\n\nplt.figure(figsize=(7, 7))\n\nplt.scatter(\n    cell_stats[\"lon_center\"],\n    cell_stats[\"lat_center\"],\n    c=cell_stats[\"smoothed_event_rate\"],\n    cmap=\"viridis\",\n    s=cell_stats[\"cell_exposure\"] / 5,\n    alpha=0.8\n)\n\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"Smoothed Spatial Risk by Grid Cell\")\nplt.colorbar(label=\"Smoothed event rate\")\nplt.show()\n\n\n\n\n\n\n\n\nComparing individual‚Äëlevel and aggregated spatial visualisations highlights the trade‚Äëoff between noise and signal. While raw location data is highly variable, spatial aggregation and smoothing reveal stable geographic patterns suitable for use in predictive models."
  },
  {
    "objectID": "projects/geospatial-risk.html#thiessen-voronoi-polygons-for-spatial-risk",
    "href": "projects/geospatial-risk.html#thiessen-voronoi-polygons-for-spatial-risk",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Thiessen (Voronoi) Polygons for Spatial Risk",
    "text": "Thiessen (Voronoi) Polygons for Spatial Risk\nTo further visualise spatial structure, we construct Thiessen (Voronoi) polygons based on spatial cell centroids. Each polygon represents the region of space closest to a given cell, providing a continuous spatial partition coloured by smoothed risk.\nFor the curious, click here to learn more about Voronoi polygons\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import Voronoi\n\n# Prepare centroid coordinates\npoints = np.column_stack([\n    cell_stats[\"lon_center\"].values,\n    cell_stats[\"lat_center\"].values\n])\n\n# Compute Voronoi tessellation\nvor = Voronoi(points)\n\n# Plot\nplt.figure(figsize=(8, 8))\n\n# Plot Voronoi regions\nfor region_index, region in enumerate(vor.regions):\n    if not region or -1 in region:\n        continue  # Skip infinite regions\n\n    polygon = [vor.vertices[i] for i in region]\n    polygon = np.array(polygon)\n\n    # Find corresponding point index\n    point_indices = np.where(vor.point_region == region_index)[0]\n    if len(point_indices) == 0:\n        continue\n\n    idx = point_indices[0]\n    risk_value = cell_stats.iloc[idx][\"smoothed_event_rate\"]\n\n    plt.fill(\n        polygon[:, 0],\n        polygon[:, 1],\n        color=plt.cm.viridis(risk_value / cell_stats[\"smoothed_event_rate\"].max()),\n        alpha=0.8\n    )\n\n# Overlay centroids\nplt.scatter(\n    cell_stats[\"lon_center\"],\n    cell_stats[\"lat_center\"],\n    c=\"black\",\n    s=10,\n    alpha=0.6\n)\n\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.title(\"Thiessen (Voronoi) Polygons Coloured by Smoothed Risk\")\nplt.show()"
  },
  {
    "objectID": "projects/geospatial-risk.html#mapping-spatial-risk-using-the-british-national-grid",
    "href": "projects/geospatial-risk.html#mapping-spatial-risk-using-the-british-national-grid",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Mapping Spatial Risk Using the British National Grid",
    "text": "Mapping Spatial Risk Using the British National Grid\nTo visualise spatial risk in a cartographically appropriate coordinate system, we project locations onto the British National Grid (BNG). This projection is commonly used for mapping and spatial analysis within Great Britain, providing distance‚Äëpreserving coordinates in metres.\nNorthern Ireland uses a separate national grid and is therefore excluded from this visualisation for technical correctness.\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Load Natural Earth country boundaries (110m resolution)\nurl = \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n\nworld = gpd.read_file(url)\n\n# Extract United Kingdom\nuk = world[world[\"NAME\"] == \"United Kingdom\"]\n\nuk\n\n\n\n\n\n\n\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\n...\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n\n143\nAdmin-0 country\n1\n2\nUnited Kingdom\nGB1\n1\n2\nCountry\n1\nUnited Kingdom\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((-6.19788 53.86757, -6.95373 54...\n\n\n\n\n1 rows √ó 169 columns\n\n\n\n\n\n# Convert point data to GeoDataFrame\ngeometry = [\n    Point(lon, lat)\n    for lon, lat in zip(df[\"longitude\"], df[\"latitude\"])\n]\n\ngdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n\n# Reproject to British National Grid (EPSG:27700)\nuk_bng = uk.to_crs(epsg=27700)\n\nuk_bng\n\n\n\n\n\n\n\n\nfeaturecla\nscalerank\nLABELRANK\nSOVEREIGNT\nSOV_A3\nADM0_DIF\nLEVEL\nTYPE\nTLC\nADMIN\n...\nFCLASS_TR\nFCLASS_ID\nFCLASS_PL\nFCLASS_GR\nFCLASS_IT\nFCLASS_NL\nFCLASS_SE\nFCLASS_BD\nFCLASS_UA\ngeometry\n\n\n\n\n143\nAdmin-0 country\n1\n2\nUnited Kingdom\nGB1\n1\n2\nCountry\n1\nUnited Kingdom\n...\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nMULTIPOLYGON (((124122.565 449434, 76073.084 4...\n\n\n\n\n1 rows √ó 169 columns"
  },
  {
    "objectID": "projects/geospatial-risk.html#keep-only-great-britain-approximate-longitude-filter",
    "href": "projects/geospatial-risk.html#keep-only-great-britain-approximate-longitude-filter",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Keep only Great Britain (approximate longitude filter)",
    "text": "Keep only Great Britain (approximate longitude filter)\n\nuk_gb = uk[uk.geometry.centroid.x &gt; -8]\n\nuk_gb_bng = uk_gb.to_crs(epsg=27700)\n\nC:\\Users\\morri\\AppData\\Local\\Temp\\ipykernel_16724\\2014797042.py:1: UserWarning:\n\nGeometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n\n\n\n\n\nfrom shapely.geometry import Point\nimport geopandas as gpd\n\ngeometry = [\n    Point(lon, lat)\n    for lon, lat in zip(df[\"longitude\"], df[\"latitude\"])\n]\n\ngdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\ngdf_bng = gdf.to_crs(epsg=27700)\n\ngdf_bng.head()\n\n\n\n\n\n\n\n\nlatitude\nlongitude\nspatial_risk\nevent\nlat_bin\nlon_bin\nspatial_cell\nsmoothed_event_rate\ngeometry\n\n\n\n\n0\n53.830581\n-4.868714\n0.006423\n0\n215\n-19\n215_-19\n0.014652\nPOINT (211310.296 440963.865)\n\n\n1\n51.573766\n-1.235600\n0.195246\n0\n206\n-4\n206_-4\n0.027660\nPOINT (453071.279 186376.537)\n\n\n2\n51.247683\n-2.048911\n0.099452\n0\n204\n-8\n204_-8\n0.041006\nPOINT (396682.661 149835.711)\n\n\n3\n53.032231\n-0.370855\n0.092219\n0\n212\n-1\n212_-1\n0.028019\nPOINT (509347.525 349568.357)\n\n\n4\n53.957079\n-2.130276\n0.387473\n0\n215\n-8\n215_-8\n0.028388\nPOINT (391549.07 451228.895)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 10))\n\n# Set subtle background colour\nax.set_facecolor(\"#f0f0f0\")\n\n# Plot spatial risk points first\ngdf_bng.plot(\n    ax=ax,\n    column=\"smoothed_event_rate\",\n    cmap=\"viridis\",\n    markersize=5,\n    alpha=0.7,\n    legend=True\n)\n\n# Plot GB boundary ON TOP as white outline\nuk_gb_bng.plot(\n    ax=ax,\n    facecolor=\"none\",\n    edgecolor=\"white\",\n    linewidth=1.5\n)\n\nax.set_title(\"Smoothed Spatial Risk (British National Grid, Great Britain)\")\nax.set_xlabel(\"Easting (m)\")\nax.set_ylabel(\"Northing (m)\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCountry boundaries are sourced from Natural Earth (public domain) and projected into the British National Grid (EPSG:27700). Northern Ireland is excluded due to its use of a separate national grid reference system."
  },
  {
    "objectID": "projects/geospatial-risk.html#wip---visualisation",
    "href": "projects/geospatial-risk.html#wip---visualisation",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "WIP - Visualisation",
    "text": "WIP - Visualisation\n\n\nimport geopandas as gpd\n\nlsoa = gpd.read_file(r\"C:\\Users\\morri\\PycharmProjects\\shmers.github.io\\geodata\\Lower_layer_Super_Output_Areas_December_2021_Boundaries_EW_BGC_V5_-6970154227154374572.gpkg\")\n\nlsoa.head()\n\n\n\n\n\n\n\n\nLSOA21CD\nLSOA21NM\nLSOA21NMW\nBNG_E\nBNG_N\nLAT\nLONG\nGlobalID\ngeometry\n\n\n\n\n0\nE01000001\nCity of London 001A\n\n532123\n181632\n51.518169\n-0.097150\n{86214465-5CF4-4E8F-9492-3667471C42D6}\nMULTIPOLYGON (((532105.312 182010.574, 532104....\n\n\n1\nE01000002\nCity of London 001B\n\n532480\n181715\n51.518829\n-0.091970\n{CD40C491-6567-405F-8C18-426E17B356CE}\nMULTIPOLYGON (((532634.497 181926.016, 532572....\n\n\n2\nE01000003\nCity of London 001C\n\n532239\n182033\n51.521740\n-0.095330\n{7FD27AAF-D858-4E46-9099-92B43F66B948}\nMULTIPOLYGON (((532135.138 182198.131, 532071....\n\n\n3\nE01000005\nCity of London 001E\n\n533581\n181283\n51.514690\n-0.076280\n{7E76A16A-028F-4F49-84B5-6E5A67322F3C}\nMULTIPOLYGON (((533808.018 180767.774, 533842....\n\n\n4\nE01000006\nBarking and Dagenham 016A\n\n544994\n184274\n51.538750\n0.089317\n{25AB047E-6FCF-4F76-9176-E92E44C0E097}\nMULTIPOLYGON (((545122.049 184314.931, 545118...."
  },
  {
    "objectID": "projects/geospatial-risk.html#reproject-to-bng",
    "href": "projects/geospatial-risk.html#reproject-to-bng",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Reproject to BNG",
    "text": "Reproject to BNG\n\nlsoa_bng = lsoa.to_crs(epsg=27700)\n\nlsoa_bng.crs\n\n&lt;Projected CRS: EPSG:27700&gt;\nName: OSGB36 / British National Grid\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: United Kingdom (UK) - offshore to boundary of UKCS within 49¬∞45'N to 61¬∞N and 9¬∞W to 2¬∞E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\n- bounds: (-9.01, 49.75, 2.01, 61.01)\nCoordinate Operation:\n- name: British National Grid\n- method: Transverse Mercator\nDatum: Ordnance Survey of Great Britain 1936\n- Ellipsoid: Airy 1830\n- Prime Meridian: Greenwich\n\n\n\nLSOA boundary data are sourced from the UK Office for National Statistics Open Geography Portal and stored locally to ensure reproducibility. This avoids reliance on unstable external URLs while preserving authoritative geography under the Open Government Licence."
  },
  {
    "objectID": "projects/geospatial-risk.html#load-population-csv",
    "href": "projects/geospatial-risk.html#load-population-csv",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Load population CSV",
    "text": "Load population CSV\n\npop = pd.read_csv(r\"C:\\Users\\morri\\PycharmProjects\\shmers.github.io\\geodata\\Mid 2022 LSOA 2021.csv\")\nprint(pop.columns.tolist())\n\npop.head()\n\n['LAD 2023 Code', 'LAD 2023 Name', 'LSOA 2021 Code', 'LSOA 2021 Name', 'Total', 'F0 to 15', 'F16 to 29', 'F30 to 44', 'F45 to 64', 'F65 and over', 'M0 to 15', 'M16 to 29', 'M30 to 44', 'M45 to 64', 'M65 and over']\n\n\n\n\n\n\n\n\n\nLAD 2023 Code\nLAD 2023 Name\nLSOA 2021 Code\nLSOA 2021 Name\nTotal\nF0 to 15\nF16 to 29\nF30 to 44\nF45 to 64\nF65 and over\nM0 to 15\nM16 to 29\nM30 to 44\nM45 to 64\nM65 and over\n\n\n\n\n0\nE06000001\nHartlepool\nE01011949\nHartlepool 009A\n1,876\n182\n166\n189\n274\n179\n189\n181\n152\n236\n128\n\n\n1\nE06000001\nHartlepool\nE01011950\nHartlepool 008A\n1,117\n96\n79\n102\n147\n99\n104\n92\n138\n175\n85\n\n\n2\nE06000001\nHartlepool\nE01011951\nHartlepool 007A\n1,260\n90\n126\n139\n170\n90\n133\n100\n146\n189\n77\n\n\n3\nE06000001\nHartlepool\nE01011952\nHartlepool 002A\n1,635\n193\n134\n156\n207\n196\n194\n109\n104\n202\n140\n\n\n4\nE06000001\nHartlepool\nE01011953\nHartlepool 002B\n1,984\n216\n220\n190\n231\n145\n290\n177\n160\n232\n123"
  },
  {
    "objectID": "projects/geospatial-risk.html#extract-relevant-columns",
    "href": "projects/geospatial-risk.html#extract-relevant-columns",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "Extract relevant columns",
    "text": "Extract relevant columns\n\npop = pop.rename(columns={\n    \"LSOA 2021 Code\": \"LSOA21CD\",\n    \"Total\": \"population\"\n})\n\npop = pop[[\"LSOA21CD\", \"population\"]]\n\npop.head()\n\n\n\n\n\n\n\n\nLSOA21CD\npopulation\n\n\n\n\n0\nE01011949\n1,876\n\n\n1\nE01011950\n1,117\n\n\n2\nE01011951\n1,260\n\n\n3\nE01011952\n1,635\n\n\n4\nE01011953\n1,984\n\n\n\n\n\n\n\n\npop[\"population\"] = (\n    pop[\"population\"]\n    .str.replace(\",\", \"\", regex=False)\n    .astype(int)\n)\n\npop.dtypes\n\nLSOA21CD      object\npopulation     int64\ndtype: object\n\n\n\nlsoa = lsoa.merge(\n    pop,\n    on=\"LSOA21CD\",\n    how=\"left\"\n)\n\nlsoa[\"population\"].isna().mean()\n\nnp.float64(0.0)"
  },
  {
    "objectID": "projects/geospatial-risk.html#project-compute-population-density",
    "href": "projects/geospatial-risk.html#project-compute-population-density",
    "title": "Incorporating Geospatial Context into Risk Modelling",
    "section": "## Project & Compute Population Density",
    "text": "## Project & Compute Population Density\n\nlsoa_bng = lsoa.to_crs(epsg=27700)\n\nlsoa_bng[\"area_km2\"] = lsoa_bng.geometry.area / 1e6\nlsoa_bng[\"pop_density\"] = (\n    lsoa_bng[\"population\"] / lsoa_bng[\"area_km2\"]\n)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(9, 11))\nax.set_facecolor(\"#f0f0f0\")\n\nlsoa_bng.plot(\n    ax=ax,\n    column=\"pop_density\",\n    cmap=\"viridis\",\n    linewidth=0,\n    legend=True,\n    legend_kwds={\n        \"label\": \"Population density (people per km¬≤)\",\n        \"shrink\": 0.6\n    }\n)\n\nax.set_title(\"Population Density by LSOA (Great Britain)\")\nax.set_axis_off()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Morris Jones",
    "section": "",
    "text": "I am a senior data scientist with over 10 years of experience designing, validating, and deploying predictive models in production environments. My background spans insurance, reinsurance, and financial services, with a focus on risk modelling, probability estimation, and data‚Äëdriven decision‚Äëmaking under uncertainty.\nThis site presents a selection of IP‚Äësafe, self‚Äëcontained demonstration projects that illustrate my approach to modelling, evaluation, and communication."
  },
  {
    "objectID": "index.html#senior-data-scientist",
    "href": "index.html#senior-data-scientist",
    "title": "Morris Jones",
    "section": "",
    "text": "I am a senior data scientist with over 10 years of experience designing, validating, and deploying predictive models in production environments. My background spans insurance, reinsurance, and financial services, with a focus on risk modelling, probability estimation, and data‚Äëdriven decision‚Äëmaking under uncertainty.\nThis site presents a selection of IP‚Äësafe, self‚Äëcontained demonstration projects that illustrate my approach to modelling, evaluation, and communication."
  },
  {
    "objectID": "index.html#areas-of-focus",
    "href": "index.html#areas-of-focus",
    "title": "Morris Jones",
    "section": "Areas of Focus",
    "text": "Areas of Focus\n\nPredictive risk and pricing models\n\nRare‚Äëevent modelling and probability calibration\n\nModel validation, monitoring, and governance\n\nApplied machine learning in regulated environments"
  },
  {
    "objectID": "index.html#featured-project",
    "href": "index.html#featured-project",
    "title": "Morris Jones",
    "section": "Featured Project",
    "text": "Featured Project\n\nDesigning a Predictive Risk Model for High‚ÄëValue Assets\nAn end‚Äëto‚Äëend demonstration of a rare‚Äëevent risk modelling workflow, covering:\n\nSynthetic data generation\nDomain‚Äëinformed feature engineering\nBaseline modelling and interpretation\nCalibration, Brier score, and Brier Skill Score\nPost‚Äëhoc probability calibration\n\nüëâ View project"
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Morris Jones",
    "section": "Get in Touch",
    "text": "Get in Touch\nIf you‚Äôre interested in discussing senior data science opportunities or would like to talk through any of the work shown here, feel free to get in touch.\n\nüìß morris.jones123@gmail.com\nüîó GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Outside of work, I enjoy chess, music, and endurance sports. I find these interests complement analytical work by encouraging long‚Äëterm thinking, focus, and resilience.\nI‚Äôm a 3d printing enthusiast, speculative fiction reader, and devoted cat dad.\nDuring my 2025 career break I‚Äôve also become an amateur android app developer (so far for brain training apps and logic / puzzle games)."
  },
  {
    "objectID": "projects/fca-motor-pricing.html",
    "href": "projects/fca-motor-pricing.html",
    "title": "Motor Pricing FCA Remediation",
    "section": "",
    "text": "The full FCA remediation plan, including delivery timeline, governance approach, and model validation evidence, is available as a downloadable PDF.\n‚û°Ô∏è Download the Motor Pricing FCA Remediation Plan (PDF)\n\n\n\n\nAudience: Senior Management, Risk Committee, FCA\nVersion: v1.0\nLast updated: 2026-01-15\nStatus: Final\n\n\nThis document is an example presentation to senior management in response to a hypothetical FCA review scenario."
  },
  {
    "objectID": "projects/fca-motor-pricing.html#download-the-full-remediation-plan",
    "href": "projects/fca-motor-pricing.html#download-the-full-remediation-plan",
    "title": "Motor Pricing FCA Remediation",
    "section": "",
    "text": "The full FCA remediation plan, including delivery timeline, governance approach, and model validation evidence, is available as a downloadable PDF.\n‚û°Ô∏è Download the Motor Pricing FCA Remediation Plan (PDF)\n\n\n\n\nAudience: Senior Management, Risk Committee, FCA\nVersion: v1.0\nLast updated: 2026-01-15\nStatus: Final\n\n\nThis document is an example presentation to senior management in response to a hypothetical FCA review scenario."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Below is a selection of self‚Äëcontained, IP‚Äësafe demonstration projects showcasing my approach to data science, modelling, and decision‚Äëmaking in a production‚Äëstyle context.\n\nDesigning a Predictive Risk Model for High‚ÄëValue Assets\nGeospatial Enhancements to Risk Modelling\nFCA Review Decision Response Plan & Commercial Mitigation Presentation (Hypothetical Scenario)"
  }
]